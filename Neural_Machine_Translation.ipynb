{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgXGcRHcaf4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import collections\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzG7r_gYY5wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7MAT_jbaJkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(path):\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "    return data.split('\\n')\n",
        "\n",
        "def process_data(data):\n",
        "    new_sentence = []\n",
        "    for sentence in data:\n",
        "        clean = re.compile('<.*>')\n",
        "        new_sentence.append(re.sub(clean, '', sentence))\n",
        "    del new_sentence[-2:]\n",
        "    return new_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DvflehKbt1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english = process_data(load_data('/data/small_vocab_en.txt'))\n",
        "french = process_data(load_data('/data/small_vocab_fr.txt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDbmV1Gbx5ZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "21aea5dd-41f3-4daa-c25a-19e11e57bbbb"
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english for word in sentence.split(\" \")])\n",
        "print(\"Total no. of words in english: \", len([word for sentence in english for word in sentence.split(\" \")]))\n",
        "print(\"Total no. of unique words: \", len(english_words_counter))\n",
        "\n",
        "french_words_counter = collections.Counter([word for sentence in french for word in sentence.split(\" \")])\n",
        "print(\"Total no. of words in french: \", len([word for sentence in french for word in sentence.split(\" \")]))\n",
        "print(\"Total no. of unique words: \", len(french_words_counter))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no. of words in english:  1823292\n",
            "Total no. of unique words:  228\n",
            "Total no. of words in french:  1961298\n",
            "Total no. of unique words:  356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pmubzcQyRCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(x):\n",
        "    tk = Tokenizer(char_level = False)\n",
        "    tk.fit_on_texts(x)\n",
        "    return tk.texts_to_sequences(x), tk\n",
        "\n",
        "def pad(x, length=None):\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen = length, padding = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ly8GJSt2cQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(x, y):\n",
        "    data_x, x_tk = tokenize(x)\n",
        "    data_y, y_tk = tokenize(y)\n",
        "\n",
        "    data_x = pad(data_x)\n",
        "    data_y = pad(data_y)\n",
        "\n",
        "    data_y = data_y.reshape(*data_y.shape, 1)\n",
        "\n",
        "    return data_x, data_y, x_tk, y_tk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4WUXPQK2fKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_english, processed_french, english_tokenizer, french_tokenizer = preprocess(english, french)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8HzpTbL3pKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b94aa6a-a406-4421-c471-0827415f59a0"
      },
      "source": [
        "max_english_sequence_length = processed_english.shape[1]\n",
        "max_french_sequence_length = processed_french.shape[1]\n",
        "print(max_english_sequence_length)\n",
        "print(max_french_sequence_length)\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n",
            "21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9p1487F53nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBNpB3zX9AAv",
        "colab_type": "text"
      },
      "source": [
        "#**Simple RNN**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fghr4Z1s6PyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_model(input_shape, french_vocab_size):\n",
        "    input_seq = Input(input_shape[1:])\n",
        "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
        "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
        "\n",
        "    model = Model(input_seq, Activation('softmax')(logits))\n",
        "    model.compile(loss = sparse_categorical_crossentropy, optimizer = Adam(1e-3), metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBE1hlmJ2dg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_x = pad(processed_english, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, processed_french.shape[-2], 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKLyZ3K74TCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_rnn_model = simple_model(tmp_x.shape, french_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frg8CuM05MnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_rnn_model.fit(tmp_x, processed_french, batch_size=1024, epochs=30, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBxC91Qa6-Zm",
        "colab_type": "code",
        "outputId": "ed7ba6ee-c8da-41d4-da4b-b286537cedc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new jersey est parfois chaud en l' et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2UM6O0PGEMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl6BsgiW9M0Q",
        "colab_type": "text"
      },
      "source": [
        "#**Using Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6jwTPqz-ge4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_model(input_shape, french_vocab_size):\n",
        "    embedding = Embedding(french_vocab_size, 64, input_length = input_shape[1])\n",
        "    rnn = GRU(64, return_sequences = True, activation='tanh')\n",
        "    logits = TimeDistributed(Dense(french_vocab_size, activation='softmax'))\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(embedding)\n",
        "    model.add(rnn)\n",
        "    model.add(logits)\n",
        "    \n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(1e-3),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuEisCk-BRkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_x = pad(processed_english, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, processed_french.shape[-2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbDdfeoPCTs8",
        "colab_type": "code",
        "outputId": "675a6c49-d4b6-4790-d1f6-ce16470575bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tmp_x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(137860, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDzF9NoWBtaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_embedd_model = embedding_model(tmp_x.shape, french_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCnEdoukC9iJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_embedd_model.fit(tmp_x, processed_french, batch_size=1024, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZW9a1dXDlkI",
        "colab_type": "code",
        "outputId": "bbc044ce-812e-4e45-a8ff-e3c16b921743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(logits_to_text(simple_embedd_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new jersey est parfois calme en l' et il il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuUBViUXDiz8",
        "colab_type": "text"
      },
      "source": [
        "#**Bidirectional RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh52hqqbEQR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_bidirectional_model(input_shape, french_vocab_size):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    bdd = Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), input_shape = input_shape[1:])\n",
        "    logits = TimeDistributed(Dense(french_vocab_size, activation='softmax'))\n",
        "    \n",
        "    model.add(bdd)\n",
        "    model.add(logits)\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(1e-3), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC_N_n8ZD5SP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_x = pad(processed_english, processed_french.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, processed_french.shape[-2], 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF__-dGbY3cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_bdd_model = simple_bidirectional_model(tmp_x.shape,  french_vocab_size+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJII7LBsZUNL",
        "colab_type": "code",
        "outputId": "f6f4ec4a-d5be-4d5e-f714-13114b13bfc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "simple_bdd_model.fit(tmp_x, processed_french, batch_size=1024, epochs=20, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27572 samples\n",
            "Epoch 1/20\n",
            "110288/110288 [==============================] - 10s 90us/sample - loss: 2.7143 - acc: 0.4996 - val_loss: 1.7922 - val_acc: 0.5702\n",
            "Epoch 2/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 1.6573 - acc: 0.5871 - val_loss: 1.4806 - val_acc: 0.6105\n",
            "Epoch 3/20\n",
            "110288/110288 [==============================] - 8s 74us/sample - loss: 1.4506 - acc: 0.6124 - val_loss: 1.3594 - val_acc: 0.6216\n",
            "Epoch 4/20\n",
            "110288/110288 [==============================] - 8s 74us/sample - loss: 1.3456 - acc: 0.6278 - val_loss: 1.2935 - val_acc: 0.6343\n",
            "Epoch 5/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 1.2755 - acc: 0.6405 - val_loss: 1.2603 - val_acc: 0.6358\n",
            "Epoch 6/20\n",
            "110288/110288 [==============================] - 8s 74us/sample - loss: 1.2226 - acc: 0.6513 - val_loss: 1.2419 - val_acc: 0.6335\n",
            "Epoch 7/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 1.1779 - acc: 0.6595 - val_loss: 1.2525 - val_acc: 0.6278\n",
            "Epoch 8/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 1.1404 - acc: 0.6664 - val_loss: 1.2590 - val_acc: 0.6266\n",
            "Epoch 9/20\n",
            "110288/110288 [==============================] - 8s 74us/sample - loss: 1.1075 - acc: 0.6719 - val_loss: 1.2686 - val_acc: 0.6230\n",
            "Epoch 10/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 1.0801 - acc: 0.6763 - val_loss: 1.2823 - val_acc: 0.6240\n",
            "Epoch 11/20\n",
            "110288/110288 [==============================] - 8s 74us/sample - loss: 1.0548 - acc: 0.6806 - val_loss: 1.3180 - val_acc: 0.6227\n",
            "Epoch 12/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 1.0334 - acc: 0.6839 - val_loss: 1.3306 - val_acc: 0.6189\n",
            "Epoch 13/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 1.0145 - acc: 0.6870 - val_loss: 1.3736 - val_acc: 0.6156\n",
            "Epoch 14/20\n",
            "110288/110288 [==============================] - 8s 74us/sample - loss: 0.9979 - acc: 0.6895 - val_loss: 1.3766 - val_acc: 0.6141\n",
            "Epoch 15/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 0.9827 - acc: 0.6923 - val_loss: 1.3950 - val_acc: 0.6153\n",
            "Epoch 16/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 0.9701 - acc: 0.6937 - val_loss: 1.4205 - val_acc: 0.6151\n",
            "Epoch 17/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 0.9571 - acc: 0.6965 - val_loss: 1.4857 - val_acc: 0.6081\n",
            "Epoch 18/20\n",
            "110288/110288 [==============================] - 8s 74us/sample - loss: 0.9450 - acc: 0.6986 - val_loss: 1.4872 - val_acc: 0.6117\n",
            "Epoch 19/20\n",
            "110288/110288 [==============================] - 8s 74us/sample - loss: 0.9342 - acc: 0.7009 - val_loss: 1.5337 - val_acc: 0.6068\n",
            "Epoch 20/20\n",
            "110288/110288 [==============================] - 8s 75us/sample - loss: 0.9213 - acc: 0.7033 - val_loss: 1.5768 - val_acc: 0.6053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f84049b5f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGJqTkqWZor6",
        "colab_type": "code",
        "outputId": "9e59b758-7464-4865-e904-eb7b5fed5e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(logits_to_text(simple_bdd_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new jersey est parfois occupé en printemps mais il est agréable en mai <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5QVJzAke6y1",
        "colab_type": "text"
      },
      "source": [
        "#***Encoder Decoder***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b-oJI2AcJtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encdec_model(input_shape, output_sequence_length, french_vocab_size):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(GRU(128, return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
        "\n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(1e-3), \n",
        "                 metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cfcIuP6cmvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_x = pad(processed_english)\n",
        "tmp_x = tmp_x.reshape((-1, processed_english.shape[1], 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxiaRCNecrCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enco_deco_model = encdec_model(\n",
        "    tmp_x.shape,\n",
        "    processed_french.shape[1],\n",
        "    french_vocab_size+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYOsuQgKc1Cd",
        "colab_type": "code",
        "outputId": "71804758-67a1-408d-eca2-1d18f1cfbdf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "encodeco_model.fit(tmp_x, processed_french, batch_size=1024, epochs=20, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27572 samples\n",
            "Epoch 1/20\n",
            "110288/110288 [==============================] - 9s 83us/sample - loss: 3.0040 - acc: 0.4378 - val_loss: 2.4432 - val_acc: 0.4962\n",
            "Epoch 2/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 2.3119 - acc: 0.5013 - val_loss: 2.2185 - val_acc: 0.5103\n",
            "Epoch 3/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 2.1327 - acc: 0.5146 - val_loss: 2.0165 - val_acc: 0.5242\n",
            "Epoch 4/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 1.9062 - acc: 0.5359 - val_loss: 1.8225 - val_acc: 0.5492\n",
            "Epoch 5/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.7599 - acc: 0.5589 - val_loss: 1.7151 - val_acc: 0.5634\n",
            "Epoch 6/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 1.6578 - acc: 0.5695 - val_loss: 1.6239 - val_acc: 0.5725\n",
            "Epoch 7/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 1.5832 - acc: 0.5812 - val_loss: 1.5484 - val_acc: 0.5849\n",
            "Epoch 8/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 1.5199 - acc: 0.5918 - val_loss: 1.4907 - val_acc: 0.5983\n",
            "Epoch 9/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 1.4651 - acc: 0.6032 - val_loss: 1.4389 - val_acc: 0.6092\n",
            "Epoch 10/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.4214 - acc: 0.6116 - val_loss: 1.4041 - val_acc: 0.6136\n",
            "Epoch 11/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.3882 - acc: 0.6171 - val_loss: 1.3723 - val_acc: 0.6206\n",
            "Epoch 12/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 1.3590 - acc: 0.6229 - val_loss: 1.3464 - val_acc: 0.6255\n",
            "Epoch 13/20\n",
            "110288/110288 [==============================] - 7s 66us/sample - loss: 1.3393 - acc: 0.6265 - val_loss: 1.3248 - val_acc: 0.6302\n",
            "Epoch 14/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.3195 - acc: 0.6306 - val_loss: 1.3206 - val_acc: 0.6281\n",
            "Epoch 15/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.3054 - acc: 0.6321 - val_loss: 1.2961 - val_acc: 0.6347\n",
            "Epoch 16/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.2944 - acc: 0.6333 - val_loss: 1.2852 - val_acc: 0.6349\n",
            "Epoch 17/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.2808 - acc: 0.6352 - val_loss: 1.2787 - val_acc: 0.6370\n",
            "Epoch 18/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.2695 - acc: 0.6376 - val_loss: 1.2675 - val_acc: 0.6389\n",
            "Epoch 19/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.2612 - acc: 0.6390 - val_loss: 1.2493 - val_acc: 0.6415\n",
            "Epoch 20/20\n",
            "110288/110288 [==============================] - 7s 67us/sample - loss: 1.2507 - acc: 0.6405 - val_loss: 1.2442 - val_acc: 0.6415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f84036dd518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xpSoRbUVwC-",
        "colab_type": "text"
      },
      "source": [
        "# *Custom*\n",
        "\n",
        "Create a model that incorporates embedding and a bidirectional RNN into one model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSMG6Uq8c_8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    embed_layer = Embedding(input_dim = english_vocab_size, output_dim = 128, input_length = input_shape[1])\n",
        "    bd_layer_1 = Bidirectional(GRU(256, return_sequences = False))\n",
        "    repeat_vector_1 = RepeatVector(output_sequence_length)\n",
        "    bd_layer_2 = Bidirectional(GRU(256, return_sequences = True))\n",
        "    time_distributed = TimeDistributed(Dense(french_vocab_size, activation = 'softmax'))\n",
        "    \n",
        "    model.add(embed_layer)\n",
        "    model.add(bd_layer_1)\n",
        "    model.add(repeat_vector_1)\n",
        "    model.add(bd_layer_2)\n",
        "    model.add(time_distributed)\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(5e-3), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRXEDNBVYu4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_x = pad(processed_english)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXbG_wFXZPyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "242c910f-7a02-49d8-c9e7-571c47a175c8"
      },
      "source": [
        "custom_model = final_model(tmp_x.shape, processed_french.shape[1], english_vocab_size + 1, french_vocab_size + 1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0625 06:50:06.253511 140378541336448 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0625 06:50:06.257546 140378541336448 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0625 06:50:06.269702 140378541336448 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0625 06:50:06.271588 140378541336448 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0625 06:50:06.276547 140378541336448 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrX-U0rxZhTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "63344d24-8275-4e6c-a9d9-fae4f414ec73"
      },
      "source": [
        "custom_model.fit(tmp_x, processed_french, batch_size = 1024, epochs = 18, validation_split = 0.2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27572 samples\n",
            "110288/110288 [==============================] - 40s 363us/sample - loss: 0.0416 - acc: 0.9877 - val_loss: 0.0802 - val_acc: 0.9787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fac21d42278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laweHuXmZsLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5d6e990-885d-45b3-a1d8-fa2fee3deca7"
      },
      "source": [
        "print(logits_to_text(custom_model.predict(tmp_x[:10])[2], french_tokenizer))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "california est généralement calme en mars et il est généralement chaud en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLfoVDwKdOmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a01b97aa-c9a3-4ecf-967d-7e8c6ae1702e"
      },
      "source": [
        "la californie est généralement calme en mars et habituellement chaude en juin"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[17, 23,  1, ..., 44,  0,  0],\n",
              "       [ 5, 20, 21, ..., 51,  2, 45],\n",
              "       [22,  1,  9, ..., 34,  0,  0],\n",
              "       ...,\n",
              "       [19,  1, 10, ..., 37,  0,  0],\n",
              "       [24,  1, 10, ..., 54,  0,  0],\n",
              "       [ 5, 84,  1, ...,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwxb-nZQdZIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
